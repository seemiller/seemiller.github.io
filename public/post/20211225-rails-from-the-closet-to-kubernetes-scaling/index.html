<!DOCTYPE html>
<html lang="en-us">
    <head>
		
		
		<meta charset="UTF-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">

		<title>Rails From the Closet to Kubernetes - Scaling &middot; Tales from the Command Line</title>

		
		<link rel="stylesheet" href="/css/style.css">
		<link rel="stylesheet" href="/css/fonts.css">
		
		<link rel="icon" href="favicon.ico" />
		<link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
		<link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
		<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

		
		<link href="" rel="alternate" type="application/rss+xml" title="Tales from the Command Line" />
	</head>

    <body>
        		<nav class="nav">
			<div class="nav-container">
				<a href="/">
					
						<h2 class="nav-title">Tales from the Command Line</h2>
					
				</a>
				<ul>
    
    
</ul>
			</div>
		</nav>

        

<main>
	<div class="post">
		<div class="post-info">
    <span>Written by</span>
        
        <br>
        <span>on&nbsp;</span><time datetime="2021-12-25 16:16:14 -0700 MST">December 25, 2021</time>
</div>

		<h1 class="post-title">Rails From the Closet to Kubernetes - Scaling</h1>
<div class="post-line"></div>

		

		<h1 id="rails---from-the-closet-to-kubernetes---scaling">Rails - From the Closet To Kubernetes - Scaling</h1>
<p>Part 2. Continuing the story from <a href="https://example.com">Deploying Rails from the Closet to Kubernetes - Deploymnet</a>. Way back in 2007, our application, Pivotal Tracker, was deployed to a bare metal server in a closet in the office. This single server hosted the Rails application and the MySQL database. This situation was obviously not ideal, at least not to today&rsquo;s standards. One server, and a noisy one I came to later find out, in a closet. Before we put a label on it, it was sometimes powered off by those who couldn&rsquo;t take the noise and who did not know it was hosting Tracker. Those few times it was powered off, we scaled to, well, zero!</p>
<p>Thinking about it, scaling to zero isn&rsquo;t such a bad thing. Why pay for computing resources if you don&rsquo;t need to? On the flip side, the reverse is also true. If more resources are needed, wouldn&rsquo;t it be great if more servers could be spawned to handle the increased load? Back in the day with only one server, while we could technically scale to zero, obviously not in an ideal fashion, we had no means to easily scale up either. Scaling up meant upgrading hardware, or trying to add additional servers. The additional servers would then require a load balancer, which is even more hardware and complexity - <a href="https://www.youtube.com/watch?v=4QHHGHve_N0">not gonna happen</a>. Our easy out for the time was that Tracker was still primarily an internal application. So we didn&rsquo;t need to scale much.</p>
<p>Fast-forward a couple of years to 2010, we got serious again about moving Tracker to a SaaS model. This meant time to move out of the server closet and off that single server. Time to start scaling up. Our first move was to EngineYard, which at the time was having some scaling problems of their own. We experienced slow databases, frequent network problems and outright outages. EngineYard worked with us to remediate our problems, but it was death by a thousand cuts, and we needed to move.</p>
<p>We shortly found a home in a hybrid Cloud VM/bare metal world at BlueBox. All of our production virtual machines ran on our own 4, dedicated bare metal servers. No noisy neighbors to worry about expect for ourselves. Our database had loads of CPU, tons of memory and SSD RAID Array. The network was blazing fast too. BlueBox was our home for 5 years.</p>
<p>Having these 4 servers meant we could scale up, and get some redundancy in the process. So on each bare metal server, we ran virtual machines for:</p>
<ul>
<li>Rails application (Nginx/Passenger)</li>
<li>Ordered Delayed Job process</li>
<li>Unordered Delayed Job process</li>
<li>Solr Cloud node for search functionality</li>
</ul>
<p>There were also 2 dedicated servers for Memcached, a utility VM for random maintenance work, and a couple of bastion servers that all of our hardware was secured behind.</p>
<p>BlueBox was also the first time and place were we had an environment other than production. An entire bare metal server hosted virtual machines for a Staging environment. And we also spun up demo and beta environments too. It was great finally having a place to preview code, find bugs snd performance test outside of prod! The performance of Pivotal Tracker at BlueBox was fantastic.</p>
<p>The forced move to Amazon Web Services and CloudFoundry was quite a departure from what we had at BlueBox. In my opinion, CloudFoundry was way overkill for what we needed. The infrastructure required to just to run a CloudFoundry instance dwarfed what we had at BlueBox. The ulterior motive for running on a CloudFoundry instance was of course to dog food it. So we rolled with it since it was helping out the company with that need.</p>
<p>We quickly discovered that in order to keep the same level of performance that we had at BlueBox, we needed more virtual machines. A lot more. Twenty to thirty, instead of 4, to handle our peak loads. That&rsquo;s quite a bit of computing resources. Did we need that all the time? Of course not, just for the busy parts of the week. Did we have a means to reduce our VM count when traffic was light on the weekends? Not really, unless we manually reduced the virtual machine count. But then we&rsquo;d have to manually increased it for Monday mornings. So, we settled on just running a (for us) large number of virtual machines, and paying more than double per month from our old BlueBox environment. This helped speed a move to the more cost-effective Google Cloud Platform.</p>
<p>While our bill did go down with the GCP move, we kept tweaking our deployments. By this point, we had a pretty robust deployment pipeline set up and running with <a href="https://concourse-ci.org/">Concourse</a>. One of our long range goals was to get to a continuous deployment, with no downtime deploys. To do this, we  implemented a <a href="https://martinfowler.com/bliki/BlueGreenDeployment.html">Blue/Green</a> deploy.</p>
<p>The Blue/Green deployment was a godsend. It allowed us to roll out code whenever we wanted without impacting our customers. On the surface it was great, but underneath it had one big problem - resources. Remember that need for 20-30 VMs? Well, now we needed resources to handle 40-60 virtual machines. The Blue/Green deployment required us to fully deploy all <code>Green</code> VM&rsquo;s before switching the network route away from <code>Blue</code>. We also needed to keep <code>Blue</code> around in case of the need to rollback. And why keep it around instead of just redeploy? Our deployment times on CloudFoundry took in the range of 20-30 minutes. If there was a problem, we needed to move quick, and a 20+ minute deploy was no good. In the end, half of the pool of resources allocated to VM&rsquo;s for the web tier sat idle.</p>
<p>So how can moving to Kubernetes help us with scaling our deployments? First, the Kubernetes deployment itself. We can specify the number of replicas to run, and we can very easily scale it up or down with a <code>kubernetes scale deployment/...</code> command. Deployments under Kubernetes offer a rollout history. This history allows us to jump back to any known good version, or just quickly undo the last. Kubernetes is also able to more efficiently start up and terminate pods in the deployments, allowing for quicker switches between blue/green deploys. In the end - much, much quicker deploy times with the built-in ability to quickly rollback or manually scale our app up or down.</p>
<p>Let&rsquo;s talk about that ability to scale up or down. Having that ability is great, but it&rsquo;s manual. Wouldn&rsquo;t it be great if Kubernetes could just scale for us automatically? Kubernetes has something called the <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">Horizontal Pod Autoscaler</a>. Sounds like it might do what we want, but it&rsquo;s still in beta. If I was to start Pivotal Tracker all over again now, I would look to something like <a href="https://knative.dev/docs/">Knative</a>. Knative has the ability to scale the pods running the application based on their load. The higher the load and requests, the more pods to handle them. In the quiet times, the pods can be scaled down completely, with the application not even running. If only something like Knative was available back in the day.</p>
<p>Knative also supports revisions. Revisions are basically the same as a deployment rollout history. As you modify and change your application, Knative will track the changes so that you can see your history and even rollback if needed. Revisions are also helpful when rolling out new containers as it lets you perform blue/green deploys. You can even configure canary builds with traffic throttling.</p>
<p>A Rails app is an ideal application for deployment and scaling with Knative. The <a href="https://knative.dev/docs/serving/convert-deployment-to-knative-service/#determine-if-your-workload-is-a-good-fit-for-knative">criteria</a> for this is:</p>
<ul>
<li>All actions initiated by an HTTP request.</li>
<li>Stateless - Rails applications don&rsquo;t necessarily need to track state locally, that&rsquo;s what the database is for!</li>
<li>Do not require local storage - Rails applications can get all configuration from ConfigMap and Secret.</li>
</ul>
<p>In this post I want to show how to set up a Rails application running on Kubernetes with Knative serving demonstrating the scale to zero feature. I&rsquo;ve already created a Rails application that we can use for this, it&rsquo;s the Rails Blog application that is detailed in the <a href="https://guides.rubyonrails.org/v6.0/getting_started.html">Ruby on Rails - Getting Started</a> guide. I&rsquo;ve already followed the guide and committed it to a GitHub <a href="https://github.com/seemiller/rails-blog">repository</a>. This demonstration will be building off of the Tanzu Community Edition cluster that was deployed to AWS in <a href="https://example.com">Part 1</a>.</p>
<h2 id="setup-and-installation">Setup and Installation</h2>
<p>If you haven&rsquo;t already cloned that repo, do that now, or use your own.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">git clone git@github.com:seemiller/rails-blog.git
cd rails-blog
</code></pre></div><p>Before we install Knative, we need to first install <a href="https://projectcontour.io/">Contour</a> as a prerequisite. Contour is an open source <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">ingress</a> provider. I&rsquo;m using still using Tanzu Community Edition, so I&rsquo;ll install the Contour package.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">tanzu package install contour <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --package-name contour.community.tanzu.vmware.com <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --version 1.18.1
</code></pre></div><p>Since I am going to use a <a href="https://knative.dev/docs/serving/using-a-custom-domain/">custom domain</a>, <code>k8squid.com</code>, I&rsquo;ll need to create a configuration file for the knative-serving package and configure the Route53 DNS.</p>
<p>Create the configuration file.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cat knative-values.yaml &lt;&lt;EOF
domain:
  type: real
  name: k8squid.com
</code></pre></div><p>Get the external IP of the Envoy Load Balancer. On Route 53&rsquo;s Hosted Zone page, confgure the <code>A</code> record for your domain to use this IP.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">&gt; kubectl get service --namespace projectcontour

NAME      TYPE           CLUSTER-IP       EXTERNAL-IP                                                              PORT<span style="color:#f92672">(</span>S<span style="color:#f92672">)</span>                      AGE
contour   ClusterIP      100.66.15.181    &lt;none&gt;                                                                   8001/TCP                     3d7h
envoy     LoadBalancer   100.69.190.187   a7d4a55e...67870.us-east-1.elb.amazonaws.com   80:30949/TCP,443:32571/TCP   3d7h
</code></pre></div><p>With our prerequisites out of the way, we can install the Knative package.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">tanzu package install knative-serving <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --package-name knative-serving.community.tanzu.vmware.com <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --version 1.0.0
  --values-file knative-serving.yaml
</code></pre></div><p>In another terminal, watch the knative-serving namespace to see the pods come up.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pods --namespace knative-serving --watch
</code></pre></div><p>I still have my Rails blog application deployed from the previous walk-through. I&rsquo;ll need to remove that deployment, ingress and service before deploying the application with Knative.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl delete --filename kubernetes/rails-deployment.yaml
kubectl delete --filename kubernetes/rails-ingress.yaml
kubectl delete --filename kubernetes/rails-service.yaml
</code></pre></div><h2 id="knative-service-deployment">Knative Service Deployment</h2>
<p>With the original deployment cleaned up, we can deploy the application using Knative. For that we need a Knative service manifest. It does seem a bit odd to be deploying our application in a service instead of a deployment, but that&rsquo;s just how it is. This is basically our original deployment, now in a Knative manifest.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">&gt; cat kubernetes/rails-knative.yaml
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">serving.knative.dev/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Service</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">rails-blog</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">blog</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">template</span>:
    <span style="color:#f92672">metadata</span>:
      <span style="color:#f92672">name</span>: <span style="color:#ae81ff">rails-blog-0001</span>
    <span style="color:#f92672">spec</span>:
      <span style="color:#f92672">containers</span>:
        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">rails</span>
          <span style="color:#f92672">image</span>: <span style="color:#ae81ff">ttl.sh/seemiller/rails-blog:1d</span>
          <span style="color:#f92672">ports</span>:
          - <span style="color:#f92672">containerPort</span>: <span style="color:#ae81ff">3000</span>
          <span style="color:#f92672">env</span>:
          - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">DATABASE_URL</span>
            <span style="color:#f92672">valueFrom</span>:
              <span style="color:#f92672">secretKeyRef</span>:
                <span style="color:#f92672">name</span>: <span style="color:#ae81ff">rails-secrets</span>
                <span style="color:#f92672">key</span>: <span style="color:#ae81ff">database-url</span>
          - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">SECRET_KEY_BASE</span>
            <span style="color:#f92672">valueFrom</span>:
              <span style="color:#f92672">secretKeyRef</span>:
                <span style="color:#f92672">name</span>: <span style="color:#ae81ff">rails-secrets</span>
                <span style="color:#f92672">key</span>: <span style="color:#ae81ff">secret-key-base</span>
</code></pre></div><blockquote>
<p>Notice that the <code>.spec.template.metadata.name</code> field has a <code>-0001</code> suffix. This is the revision of the service. The revision can be used to track the different versions of your application.</p>
</blockquote>
<p>Have 2 terminal windows open - 1 to deploy the service and another to watch pods.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#75715e"># second terminal</span>
kubectl get pods --namespace<span style="color:#f92672">=</span>blog --watch
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#75715e"># primary terminal</span>
kubectl apply --filename<span style="color:#f92672">=</span>kubernetes/rails-knative-service.yaml
</code></pre></div><p>Notice that Knative will deploy 2 pods. Wait a few minutes and notice that the pods are terminated. Let&rsquo;s get the Knative service to see how to make a request to our Rails application. Notice that the URL for our application is the name of the service, the namespace, and finally our custom domain name.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">&gt; kubectl get ksvc --namespace blog

NAME         URL                                       LATESTCREATED     LATESTREADY       READY   REASON
rails-blog   http://rails-blog.blog.k8squid.com        rails-blog-0001   rails-blog-0001   True
</code></pre></div><p>Hit that URL with a browser or a curl command. Wait 30 seconds and see the pods terminate. Repeat your requests to the application and watch the pods come back to life!</p>
<h2 id="conclusion">Conclusion</h2>
<p>In my opinion, having a service that is able to spin up an app on demand and spin it down when idle is pretty amazing. Instead of wasting computing resources on idle web servers, the system can balance itself out and be available for other things. Add in the ability to manage revisions and perform blue/green/canary deploys and you&rsquo;ve got a pretty solid deployment service. Knative could have gone a lng way in helping us solve our scaling issues and perhaps simplified our deployment pipeline. If Knative serving is at all intriguing, I would recommend a review of their official <a href="https://knative.dev/docs/serving/">documentation</a> or take a look at the <a href="https://knative.tips/pod-config/volumes/">Knative: Operator’s Handbook</a> for other examples.</p>


		
	</div>

	<div class="pagination">
		<a href="/post/20211225-rails-from-the-closet-to-kubernetes-deployment/" class="left arrow">&#8592;</a>

		<a href="#" class="top">Top</a>
	</div>
</main>


        		<footer>
			
			<span>
			&copy; <time datetime="2021-12-25 17:02:31.14467 -0700 MST m=&#43;0.051305323">2021</time> . Made with <a href='https://gohugo.io'>Hugo</a> using the <a href='https://github.com/EmielH/tale-hugo/'>Tale</a> theme.
			</span>
		</footer>

    </body>
</html>
